# Agentic Framework - LLM Application with RAG Capabilities

## üìã Project Overview

This is a production-ready **Agentic Framework** built with FastAPI that provides a complete RAG (Retrieval-Augmented Generation) system for document processing and intelligent chat interactions. The framework implements a clean, layered architecture with factory patterns, dependency injection, and comprehensive state management.

### Key Features

- **Document Processing Pipeline**: Extract, chunk, embed, and store documents in a vector database
- **RAG-Enabled Chat**: Intelligent chat interface with context retrieval from uploaded documents
- **State Management**: Comprehensive caching and state tracking for document processing stages
- **Modular Architecture**: Factory pattern-based service layer for easy extensibility
- **Multiple Extractor Support**: Docling-based document extraction with support for multiple formats
- **Vector Search**: Semantic similarity search using embeddings
- **Streaming Responses**: Real-time streaming chat responses
- **Production-Ready**: Proper error handling, logging, and lifecycle management

---

## üèóÔ∏è Architecture Overview

The framework follows a **layered architecture** with clear separation of concerns:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    API Layer (Routes)                        ‚îÇ
‚îÇ  app/api/v1/chat.py, app/api/v1/documents.py               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Controller Layer                            ‚îÇ
‚îÇ  ChatController, DocumentController                         ‚îÇ
‚îÇ  - Orchestrates business logic                              ‚îÇ
‚îÇ  - Coordinates between repositories, pipelines, managers     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ              ‚îÇ              ‚îÇ
        ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Repositories ‚îÇ ‚îÇ  Pipeline    ‚îÇ ‚îÇ   Manager    ‚îÇ
‚îÇ              ‚îÇ ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ
‚îÇ DocumentRepo ‚îÇ ‚îÇ DocumentPipe ‚îÇ ‚îÇ StateManager‚îÇ
‚îÇ StateRepo    ‚îÇ ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                ‚îÇ                ‚îÇ
       ‚îÇ                ‚îÇ                ‚îÇ
       ‚ñº                ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Services Layer                              ‚îÇ
‚îÇ  BaseDB, BaseLLM, BaseEmbeddings, BaseExtractor, etc.      ‚îÇ
‚îÇ  - Implementation details                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Infrastructure Layer                            ‚îÇ
‚îÇ  Redis, Ollama, HuggingFace, etc.                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ System Flow

### 1. Document Upload & Processing Flow

```
1. API Route (POST /api/v1/upload)
   ‚îú‚îÄ> Validates file (type, size, extension)
   ‚îú‚îÄ> Computes SHA256 file hash
   ‚îî‚îÄ> Saves file temporarily

2. DocumentController.upload()
   ‚îú‚îÄ> Checks cache via StateManager
   ‚îÇ   ‚îî‚îÄ> If already processed (STORED stage), returns cached chunks
   ‚îÇ
   ‚îú‚îÄ> Calls Pipeline.process()
   ‚îÇ   ‚îú‚îÄ> Stage 1: Extract (BaseExtractor.extract())
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Converts PDF/DOCX to structured documents
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îú‚îÄ> Stage 2: Chunk (BaseChunker.chunk())
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Splits documents into semantic chunks
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Caches chunked documents in Redis
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îî‚îÄ> Updates ProcessingState (UPLOADED ‚Üí EXTRACTED ‚Üí CHUNKED)
   ‚îÇ
   ‚îî‚îÄ> DocumentRepository.add_documents()
       ‚îú‚îÄ> Generates embeddings for each chunk (BaseEmbeddings)
       ‚îî‚îÄ> Stores in vector database (Redis with embeddings)
       ‚îî‚îÄ> Updates ProcessingState to STORED
```

**Processing Stages:**
- `UPLOADED`: File received and validated
- `EXTRACTED`: Text extracted from document
- `CHUNKED`: Documents split into chunks
- `EMBEDDED`: Chunks embedded (implicit during storage)
- `STORED`: Chunks stored in vector database
- `FAILED`: Processing failed with error message

### 2. Chat with RAG Flow

```
1. API Route (GET /api/v1/chat/chat)
   ‚îú‚îÄ> Gets LLM instance (Ollama)
   ‚îú‚îÄ> Gets embeddings instance (HuggingFace)
   ‚îî‚îÄ> Creates DocumentRepository from DB

2. ChatController.chat_stream()
   ‚îú‚îÄ> Builds messages with RAG context
   ‚îÇ   ‚îú‚îÄ> DocumentRepository.similarity_search()
   ‚îÇ   ‚îÇ   ‚îú‚îÄ> Embeds user query
   ‚îÇ   ‚îÇ   ‚îú‚îÄ> Searches vector DB for top-k similar chunks
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Returns relevant document chunks
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îî‚îÄ> Constructs prompt with context:
   ‚îÇ       "Answer based on context: [retrieved chunks]
   ‚îÇ        Question: [user query]"
   ‚îÇ
   ‚îî‚îÄ> LLM.model_stream_response()
       ‚îî‚îÄ> Streams response token by token
```

---

## üß© Key Components

### 1. **Controllers** (`app/controllers/`)

**DocumentController**
- Orchestrates document upload and processing
- Manages pipeline execution
- Handles state transitions
- Coordinates between pipeline, state manager, and repository

**ChatController**
- Orchestrates chat interactions
- Builds RAG-enhanced prompts
- Manages streaming responses
- Handles optional RAG context retrieval

### 2. **Pipeline** (`app/pipeline/`)

**DocumentPipeline**
- Orchestrates the document processing pipeline
- Manages stage transitions (UPLOADED ‚Üí EXTRACTED ‚Üí CHUNKED)
- Implements caching strategy (checks cache before processing)
- Handles errors and state updates

### 3. **Repositories** (`app/repositories/`)

**DocumentRepository**
- Abstracts vector database operations
- Handles document storage with embeddings
- Implements similarity search
- Manages document deletion

**StateRepository**
- Manages processing state in cache
- Handles state persistence and retrieval
- Tracks processing stages and errors

### 4. **Managers** (`app/manager/`)

**StateManager**
- High-level state management
- Caches chunked documents
- Manages processing state lifecycle
- Handles cache invalidation

### 5. **Services** (`app/services/`)

All services follow a **Factory Pattern** with base classes:

- **Extractor** (`extractor/`): Document extraction (Docling)
- **Chunker** (`chunker/`): Document chunking (Hybrid chunker)
- **Embeddings** (`embedder/`): Text embeddings (HuggingFace)
- **LLM** (`llm/`): Language model (Ollama)
- **Database** (`db/`): Vector database (Redis)
- **Cache** (`cache/`): Caching layer (Redis)

Each service has:
- `base.py`: Abstract base class defining interface
- `factory.py`: Factory function with registry pattern
- Implementation files (e.g., `redis_db.py`, `ollama_llm.py`)

### 6. **Dependency Injection** (`app/dependency.py`)

- Singleton pattern for expensive resources (embeddings, extractor, cache)
- Lifecycle management for database connections
- Factory functions for service instantiation
- Proper cleanup and resource management

---

## üõ†Ô∏è Technology Stack

### Core Framework
- **FastAPI**: Modern, fast web framework for building APIs
- **Pydantic**: Data validation using Python type annotations
- **Python 3.10+**: Modern Python features

### LLM & AI
- **Ollama**: Local LLM inference (supports Llama2, etc.)
- **HuggingFace Transformers**: Embedding models (sentence-transformers)
- **Docling**: Advanced document extraction and parsing

### Data Storage
- **Redis**: Vector database and caching layer
- **Sentence Transformers**: Embedding generation

### Document Processing
- **Docling**: Multi-format document extraction (PDF, DOCX, etc.)
- **Hybrid Chunking**: Semantic and structural chunking

### Development Tools
- **Colorlog**: Colored logging
- **Pydantic Settings**: Environment-based configuration

---

## üìÅ Project Structure

```
agentic_framework/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v1/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ chat.py              # Chat API routes
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ documents.py         # Document API routes
‚îÇ   ‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat_controller.py       # Chat orchestration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ document_controller.py   # Document orchestration
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py              # Document processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_repository.py   # Document DB operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state_repository.py      # State cache operations
‚îÇ   ‚îú‚îÄ‚îÄ manager/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state_manager.py         # State management
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extractor/               # Document extraction services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker/                 # Chunking services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedder/                # Embedding services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/                     # LLM services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db/                      # Database services
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cache/                   # Caching services
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_model.py        # Document data models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processing_state.py      # Processing state models
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ dependency.py                # Dependency injection
‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py               # Custom exceptions
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # FastAPI application
‚îÇ   ‚îî‚îÄ‚îÄ router.py                   # Route registration
‚îú‚îÄ‚îÄ ui/                              # Streamlit UI (optional)
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îî‚îÄ‚îÄ README.md                        # This file
```

---

## üöÄ Setup & Installation

### Prerequisites

- Python 3.10+
- Redis server running (for vector DB and caching)
- Ollama installed and running (for LLM inference)

### Installation Steps

1. **Clone the repository**
   ```bash
   cd agentic_framework
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment variables**
   Create a `.env` file (optional, defaults are provided):
   ```env
   # LLM Configuration
   LLM_TYPE=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=llama2

   # Database Configuration
   DATABASE_TYPE=redis
   REDIS_URL=redis://localhost:6379
   COLLECTION_NAME=default

   # Embeddings Configuration
   EMBEDDINGS_TYPE=huggingface
   MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

   # Extractor Configuration
   EXTRACTOR_TYPE=docling
   CHUNKER_TYPE=docling-hybrid

   # RAG Configuration
   RAG_ENABLED=true
   RAG_TOP_K=5
   RAG_MIN_SCORE=0.7

   # Cache Configuration
   ENABLE_CACHING=true
   CACHE_TYPE=redis-cache
   ```

4. **Start Redis**
   ```bash
   redis-server
   ```

5. **Start Ollama** (if not already running)
   ```bash
   ollama serve
   ```

6. **Run the application**
   ```bash
   uvicorn app.main:app --reload
   ```

7. **Access the API**
   - API Documentation: http://localhost:8000/docs
   - Health Check: http://localhost:8000/health

---

## üì° API Endpoints

### Document Endpoints

#### `POST /api/v1/upload`
Upload and process a document.

**Parameters:**
- `file`: File upload (PDF, DOCX, TXT, MD)
- `forced_reprocess`: Boolean (optional, default: false)

**Response:**
```json
{
  "status": "success",
  "file_name": "document.pdf",
  "message": "Document processed and stored successfully. Created 15 chunks. (Hash: abc123...)"
}
```

#### `DELETE /api/v1/delete-all`
Delete all documents from the database.

**Response:**
```json
{
  "status": "success",
  "file_name": "",
  "message": "Successfully deleted 50 documents from database."
}
```

### Chat Endpoints

#### `GET /api/v1/chat/chat`
Chat with the LLM (with optional RAG).

**Parameters:**
- `query`: User query (required)
- `model_name`: LLM model name (optional, default: "ollama/llama2")
- `use_rag`: Enable RAG (optional, overrides config)

**Response:**
Streaming text response (text/plain)

### Health Check

#### `GET /health`
Application health check.

**Response:**
```json
{
  "message": "LLM Application",
  "status": "healthy",
  "version": "1.0.0"
}
```

---

## üéØ Design Patterns & Best Practices

### 1. **Factory Pattern**
All services use factory functions with registry patterns:
```python
# Example: Database Factory
DATABASE_REGISTRY = {
    "redis": RedisDB,
    # Easy to add more: "pinecone": PineconeDB
}

def get_db_instance() -> BaseDB:
    db_class = DATABASE_REGISTRY.get(config.DATABASE_TYPE)
    return db_class()
```

### 2. **Repository Pattern**
Data access is abstracted through repositories:
- Controllers use repositories, not services directly
- Easy to swap database implementations
- Clear separation of concerns

### 3. **Dependency Injection**
FastAPI's `Depends()` used throughout:
- Singleton pattern for expensive resources
- Proper lifecycle management
- Testable components

### 4. **State Management**
Comprehensive state tracking:
- Processing stages tracked in cache
- Chunked documents cached for performance
- Error states preserved for debugging

### 5. **Error Handling**
Custom exception hierarchy:
- `FrameworkException`: Base exception
- `DocumentProcessingError`: Document processing failures
- `DatabaseError`: Database operation failures
- `LLMError`: LLM operation failures
- `ValidationError`: Input validation failures

### 6. **Logging**
Structured logging throughout:
- Color-coded log levels
- Contextual information
- Error stack traces

---

## üîç Key Features Explained

### 1. **Intelligent Caching**
- File hash-based deduplication
- Chunked documents cached to avoid re-processing
- Processing state persisted across requests
- Configurable TTL for cache entries

### 2. **Flexible Service Layer**
- Easy to swap implementations (e.g., switch from Ollama to OpenAI)
- Factory pattern allows runtime configuration
- Base classes ensure interface consistency

### 3. **RAG Implementation**
- Semantic search using embeddings
- Top-k retrieval with similarity scoring
- Context injection into LLM prompts
- Configurable RAG parameters

### 4. **Streaming Responses**
- Real-time token streaming for chat
- Better user experience
- Lower latency

### 5. **Production-Ready Features**
- Comprehensive error handling
- Health check endpoint
- CORS configuration
- Request validation
- Lifecycle management

---

## üìä Processing State Flow

```
UPLOADED
   ‚îÇ
   ‚ñº
EXTRACTED (if not cached)
   ‚îÇ
   ‚ñº
CHUNKED (if not cached)
   ‚îÇ
   ‚ñº
STORED (after embedding & DB insertion)
   ‚îÇ
   ‚îî‚îÄ> FAILED (if any error occurs)
```

Each stage is cached and can be resumed from any point.

---

## üîß Configuration Options

Key configuration options (see `app/config.py` for full list):

- **LLM_TYPE**: LLM provider (default: "ollama")
- **DATABASE_TYPE**: Vector database (default: "redis")
- **EXTRACTOR_TYPE**: Document extractor (default: "docling")
- **CHUNKER_TYPE**: Chunking strategy (default: "docling-hybrid")
- **EMBEDDINGS_TYPE**: Embedding provider (default: "huggingface")
- **RAG_ENABLED**: Enable/disable RAG (default: true)
- **RAG_TOP_K**: Number of chunks to retrieve (default: 5)
- **CHUNK_SIZE**: Chunk size in characters (default: 1000)
- **CHUNK_OVERLAP**: Overlap between chunks (default: 200)
- **ENABLE_CACHING**: Enable caching (default: true)

---

## üß™ Testing the System

### 1. Upload a Document
```bash
curl -X POST "http://localhost:8000/api/v1/upload?forced_reprocess=false" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf"
```

### 2. Chat with RAG
```bash
curl "http://localhost:8000/api/v1/chat/chat?query=What%20is%20the%20main%20topic%20of%20the%20document?&use_rag=true"
```

### 3. Chat without RAG
```bash
curl "http://localhost:8000/api/v1/chat/chat?query=Hello%20world&use_rag=false"
```

---

## üìù Architecture Strengths

‚úÖ **Clean Separation of Concerns**: Each layer has a clear responsibility  
‚úÖ **Extensibility**: Easy to add new extractors, chunkers, or databases  
‚úÖ **Testability**: Dependency injection makes components easily testable  
‚úÖ **Performance**: Caching reduces redundant processing  
‚úÖ **Maintainability**: Clear structure and consistent patterns  
‚úÖ **Production-Ready**: Proper error handling, logging, and lifecycle management  

---

## üöß Future Enhancements

Potential improvements:
- [ ] Add more LLM providers (OpenAI, Anthropic, etc.)
- [ ] Support for more document formats
- [ ] Advanced chunking strategies
- [ ] Multi-tenant support
- [ ] Authentication and authorization
- [ ] Rate limiting
- [ ] Metrics and monitoring
- [ ] Batch document processing
- [ ] Document versioning

---

## üìÑ License

This project is part of a personal/portfolio project demonstrating advanced Python architecture patterns and LLM application development.

---

## üë§ Author

Developed as a demonstration of:
- Clean architecture principles
- Factory and repository patterns
- RAG system implementation
- Production-ready FastAPI applications
- LLM integration best practices

---

**Version**: 1.0.0  
**Last Updated**: 2024
